from flask import Blueprint, jsonify, request
from services.preprocess import preprocess_data, is_large_dataset
from services.forecast import forecast_demand, forecast_demand_by_category
from utils.cache import get_cache, set_cache
from concurrent.futures import ProcessPoolExecutor, as_completed
import time
from routes.helpers.safe_forecast import safe_forecast

forecast_bp = Blueprint("forecast", __name__, url_prefix="/forecast")


@forecast_bp.route("/demand", methods=["GET"])
def get_demand_forecast():
    return get_forecast_for_all_categories()


@forecast_bp.route("/demand/category/<category_name>", methods=["GET"])
def get_forecast_by_category(category_name):
    try:
        # Ki·ªÉm tra xem c√≥ y√™u c·∫ßu s·ª≠ d·ª•ng Spark kh√¥ng
        use_spark = request.args.get("use_spark", "false").lower() == "true"
        
        # N·∫øu kh√¥ng ch·ªâ ƒë·ªãnh, ki·ªÉm tra k√≠ch th∆∞·ªõc d·ªØ li·ªáu
        if not use_spark:
            use_spark = is_large_dataset()
            
        if use_spark:
            print(f"üöÄ S·ª≠ d·ª•ng Spark ƒë·ªÉ d·ª± b√°o cho danh m·ª•c {category_name}")
            from services.spark_analytics import forecast_demand_by_category_spark
            result = forecast_demand_by_category_spark(category_name)
        else:
            result = forecast_demand_by_category(category_name)
            
        return jsonify(result)
    except Exception as e:
        print(f"‚ùå Route error for category {category_name}: {str(e)}")
        return jsonify({
            "status": "error",
            "message": f"Error processing forecast for category {category_name}: {str(e)}",
            "forecast_table": [],
            "chart_data": []
        }), 500


@forecast_bp.route("/demand/all", methods=["GET"])
def get_forecast_for_all_categories():
    try:
        limit = int(request.args.get("limit", 15))
        cache_key = f"forecast_all_categories_{limit}"
        
        # Ki·ªÉm tra xem c√≥ y√™u c·∫ßu s·ª≠ d·ª•ng Spark kh√¥ng
        use_spark = request.args.get("use_spark", "false").lower() == "true"
        
        # Th√™m tham s·ªë force ƒë·ªÉ b·∫Øt bu·ªôc t√≠nh to√°n l·∫°i b·ªè qua cache
        force_refresh = request.args.get("force", "false").lower() == "true"
        
        # N·∫øu kh√¥ng force v√† c√≥ cache, s·ª≠ d·ª•ng cache
        cached_result = get_cache(cache_key) if not force_refresh else None
        if cached_result:
            print(f"‚úÖ Returning cached forecast for {limit} categories")
            return jsonify(cached_result)

        # N·∫øu kh√¥ng ch·ªâ ƒë·ªãnh, ki·ªÉm tra k√≠ch th∆∞·ªõc d·ªØ li·ªáu
        if not use_spark:
            use_spark = is_large_dataset()

        all_forecasts = []
            
        # S·ª≠ d·ª•ng Spark cho t·∫≠p d·ªØ li·ªáu l·ªõn
        if use_spark:
            print(f"üöÄ S·ª≠ d·ª•ng Spark ƒë·ªÉ d·ª± b√°o cho t·∫•t c·∫£ danh m·ª•c")
            from services.spark_analytics import forecast_demand_spark, forecast_demand_by_category_spark
            
            # D·ª± b√°o t·ªïng th·ªÉ
            print("üöÄ Forecasting for T·ªïng th·ªÉ v·ªõi Spark...")
            t0 = time.time()
            overall = forecast_demand_spark()
            overall["category"] = "Overall"
            all_forecasts.append(overall)
            print(f"‚úÖ Done T·ªïng th·ªÉ with Spark in {round(time.time() - t0, 2)}s")
            
            # S·ª≠ d·ª•ng Spark ƒë·ªÉ x·ª≠ l√Ω song song c√°c danh m·ª•c
            print(f"üöÄ S·ª≠ d·ª•ng Spark ƒë·ªÉ d·ª± b√°o song song cho {limit} danh m·ª•c...")
            from services.spark_analytics import forecast_all_categories_spark
            category_forecasts = forecast_all_categories_spark(limit)
            all_forecasts.extend(category_forecasts)
            
        else:
            # S·ª≠ d·ª•ng Pandas v√† ProcessPoolExecutor nh∆∞ tr∆∞·ªõc
            df = preprocess_data()
            all_categories = df["product_category_name"].dropna().unique().tolist()
            unique_categories = list(dict.fromkeys(all_categories))
            limited_categories = unique_categories[:limit]

            print("üöÄ Forecasting for T·ªïng th·ªÉ...")
            t0 = time.time()
            overall = forecast_demand()
            overall["category"] = "Overall"
            all_forecasts.append(overall)
            print(f"‚úÖ Done T·ªïng th·ªÉ in {round(time.time() - t0, 2)}s")

            print(f"üöÄ Forecasting for {len(limited_categories)} categories in parallel...")
            with ProcessPoolExecutor(max_workers=4) as executor:
                futures = {executor.submit(safe_forecast, cat): cat for cat in limited_categories}
            for future in as_completed(futures):
                category = futures[future]
                try:
                    result = future.result()
                    print(f"‚úÖ Forecast success for {category} - status: {result.get('status')}")
                    all_forecasts.append(result)
                except Exception as e:
                    print(f"‚ùå Forecast failed for {category}: {str(e)}")

        # üëâ L·ªçc ra nh·ªØng danh m·ª•c th√†nh c√¥ng
        successful_forecasts = [f for f in all_forecasts if f.get("status") == "success"]
        print(f"üìä T·ªïng c·ªông {len(successful_forecasts)}/{len(all_forecasts)} danh m·ª•c c√≥ d·ª± b√°o th√†nh c√¥ng")

        set_cache(cache_key, successful_forecasts, ttl_seconds=60 * 60)
        return jsonify(successful_forecasts)


    except Exception as e:
        print(f"‚ùå Error in /forecast/demand/all: {str(e)}")
        return jsonify({
            "status": "error",
            "message": f"Error processing all categories: {str(e)}",
            "stacktrace": str(e)
        }), 500


@forecast_bp.route("/clear-cache", methods=["POST"])
def clear_forecast_cache():
    from utils.cache import _cache_store
    _cache_store.clear()
    return jsonify({"message": "Cache cleared!"})