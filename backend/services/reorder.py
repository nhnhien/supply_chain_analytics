import pandas as pd
import numpy as np
from services.preprocess import preprocess_data, is_large_dataset
from services.forecast import forecast_demand, forecast_demand_by_category
from utils.cache import get_cache, set_cache
from utils.currency import brl_to_vnd, format_vnd
import os
from sklearn.cluster import KMeans
from services.mongodb import (
    save_reorder_strategy,
    save_reorder_recommendations,
    save_supplier_clusters,
    save_bottleneck_analysis,
)

def calculate_reorder_strategy():
    cache_key = "reorder_strategy"
    cached = get_cache(cache_key)
    if cached:
        return cached

    # Ki·ªÉm tra xem c√≥ d√πng Spark hay kh√¥ng
    use_spark = is_large_dataset()
    if use_spark:
        print("üìä S·ª≠ d·ª•ng Spark ƒë·ªÉ t√≠nh to√°n chi·∫øn l∆∞·ª£c reorder (d·ªØ li·ªáu l·ªõn)")
        from services.spark_analytics import calculate_reorder_strategy_spark
        return calculate_reorder_strategy_spark()
    
    # N·∫øu kh√¥ng d√πng Spark, ti·∫øp t·ª•c v·ªõi code hi·ªán t·∫°i
    df = preprocess_data()
    categories = df["product_category_name"].dropna().unique()

    forecast_cache_key = "forecast_all_categories_15"
    cached_forecasts = get_cache(forecast_cache_key)

    # üîÅ N·∫øu ch∆∞a c√≥ cache, t·ª± g·ªçi forecast_demand_all
    if not cached_forecasts:
        print("‚ö†Ô∏è Forecast cache ch∆∞a c√≥, t·ª± ƒë·ªông g·ªçi forecast_demand_all...")
        from routes.forecast import get_forecast_for_all_categories
        response = get_forecast_for_all_categories()
        cached_forecasts = response.get_json()

    if not cached_forecasts:
        print("‚ùå Forecast cache v·∫´n ch∆∞a s·∫µn s√†ng sau khi g·ªçi. D·ª´ng l·∫°i.")
        return []

    forecast_map = {f["category"]: f for f in cached_forecasts if f["status"] == "success"}

    z_score = 1.65
    # Chuy·ªÉn ƒë·ªïi gi√° tr·ªã t·ª´ BRL sang VND
    holding_cost_per_unit_per_month = brl_to_vnd(2)  # Gi·∫£ s·ª≠ chi ph√≠ 2 BRL/ƒë∆°n v·ªã/th√°ng
    strategy = []

    for category in categories:
        if category not in forecast_map:
            print(f"‚ö†Ô∏è B·ªè qua {category} v√¨ kh√¥ng c√≥ trong forecast cache.")
            continue

        try:
            forecast_result = forecast_map[category]
            forecast_df = pd.DataFrame(forecast_result["forecast_table"])
            forecast_df["predicted_orders"] = forecast_df["xgboost"].astype(int)
            avg_demand = forecast_df["predicted_orders"].mean()
            demand_std = forecast_df["predicted_orders"].std()

            cat_df = df[df["product_category_name"] == category]
            lead_time = cat_df["shipping_duration"].mean()
            lead_time_months = round(lead_time / 30, 2)

            safety_stock = int(z_score * demand_std * np.sqrt(lead_time)) if lead_time > 0 else 0
            reorder_point = int(avg_demand * lead_time + safety_stock)
            optimal_inventory = reorder_point + safety_stock
            holding_cost = int(optimal_inventory * holding_cost_per_unit_per_month * lead_time_months)

            strategy.append({
                "category": category,
                "avg_lead_time_days": round(lead_time, 2),
                "forecast_avg_demand": int(avg_demand),
                "demand_std": int(demand_std),
                "safety_stock": safety_stock,
                "reorder_point": reorder_point,
                "optimal_inventory": optimal_inventory,
                "holding_cost": holding_cost
            })
            print(f"‚úÖ {category} | Holding cost = {format_vnd(holding_cost)}")

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω {category}: {str(e)}")
            continue

    set_cache(cache_key, strategy, ttl_seconds=3600)
    save_reorder_strategy(strategy)
    return strategy


def generate_optimization_recommendations(strategy_data, return_df=False):
    recommendations = []
    
    strategy_df = pd.DataFrame(strategy_data)

    if "holding_cost" not in strategy_df.columns:
        print("‚ùå Kh√¥ng c√≥ c·ªôt 'holding_cost'.")
        return pd.DataFrame() if return_df else None

    strategy_df["holding_cost"] = pd.to_numeric(strategy_df["holding_cost"], errors="coerce").fillna(0).astype(int)
    high_holding_cost = strategy_df.sort_values("holding_cost", ascending=False).head(10)

    for _, row in high_holding_cost.iterrows():
        category = row["category"]
        holding_cost = row["holding_cost"]
        safety_stock = row["safety_stock"]
        reorder_point = row["reorder_point"]
        avg_lead_time_days = row.get("avg_lead_time_days", 30)
        holding_cost_per_unit = row.get("holding_cost_per_unit", brl_to_vnd(5))

        if holding_cost > 100_000:
            new_safety_stock = int(safety_stock * 0.8)
            new_reorder_point = int(reorder_point * 0.9)
            new_optimal_inventory = new_safety_stock + new_reorder_point
            new_holding_cost = int(new_optimal_inventory * holding_cost_per_unit * round(avg_lead_time_days / 30, 2))
            potential_saving = holding_cost - new_holding_cost

            recommendations.append({
                "category": category,
                "recommendation": (
                    f"Gi·∫£m Safety Stock t·ª´ {safety_stock} ‚Üí {new_safety_stock} "
                    f"v√† Reorder Point t·ª´ {reorder_point} ‚Üí {new_reorder_point} ƒë·ªÉ ti·∫øt ki·ªám chi ph√≠."
                ),
                "new_safety_stock": new_safety_stock,
                "new_reorder_point": new_reorder_point,
                "new_optimal_inventory": new_optimal_inventory,
                "new_holding_cost": new_holding_cost,
                "potential_saving": potential_saving
            })

    if not recommendations:
        print("‚ö†Ô∏è Kh√¥ng c√≥ khuy·∫øn ngh·ªã n√†o ƒë·ªß ƒëi·ªÅu ki·ªán.")
        return pd.DataFrame() if return_df else None

    save_reorder_recommendations(recommendations)

    df = pd.DataFrame(recommendations)

    # ‚úÖ Ghi file Excel ƒë·ªÉ h·ªó tr·ª£ route download
    output_dir = os.path.join("charts", "reorder")
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "optimization_recommendations.xlsx")
    df.to_excel(output_path, index=False)

    print(f"‚úÖ File khuy·∫øn ngh·ªã ƒë√£ ƒë∆∞·ª£c t·∫°o t·∫°i: {output_path}")

    return df if return_df else output_path


def cluster_suppliers(n_clusters=3):
    """
    Ph√¢n c·ª•m nh√† cung c·∫•p d·ª±a tr√™n s·ªë l∆∞·ª£ng ƒë∆°n h√†ng, th·ªùi gian giao h√†ng,
    v√† chi ph√≠ v·∫≠n chuy·ªÉn trung b√¨nh.
    """
    try:
        print("üöÄ B·∫Øt ƒë·∫ßu ph√¢n c·ª•m nh√† cung c·∫•p...")
        cache_key = "supplier_clusters"
        cached = get_cache(cache_key)
        if cached:
            return cached

        # Ki·ªÉm tra xem c√≥ n√™n d√πng Spark hay kh√¥ng
        use_spark = is_large_dataset()
        if use_spark:
            print("üìä S·ª≠ d·ª•ng Spark ƒë·ªÉ ph√¢n c·ª•m nh√† cung c·∫•p (d·ªØ li·ªáu l·ªõn)")
            from services.spark_analytics import cluster_suppliers_spark
            clusters = cluster_suppliers_spark(n_clusters)
            
            # Cache v√† l∆∞u k·∫øt qu·∫£
            set_cache(cache_key, clusters, ttl_seconds=3600*24)
            save_supplier_clusters(clusters)
            
            return clusters

        # N·∫øu kh√¥ng d√πng Spark, ti·∫øp t·ª•c v·ªõi code hi·ªán t·∫°i
        df = preprocess_data()

        supplier_df = df.groupby("seller_id").agg({
            "order_id": "nunique",
            "shipping_duration": "mean",
            "shipping_charges": "mean"  # ‚úÖ d√πng ƒë√∫ng t√™n c·ªôt
        }).reset_index()

        supplier_df.columns = ["seller_id", "total_orders", "avg_shipping_days", "avg_freight"]
        
        # ƒê·∫£m b·∫£o d·ªØ li·ªáu c√≥ ƒë√∫ng ƒë·ªãnh d·∫°ng
        supplier_df["avg_shipping_days"] = supplier_df["avg_shipping_days"].fillna(0).astype(float)
        supplier_df["avg_freight"] = supplier_df["avg_freight"].apply(lambda x: float(brl_to_vnd(x)) if pd.notnull(x) else 0)
        supplier_df["total_orders"] = supplier_df["total_orders"].fillna(0).astype(int)
        
        # L·ªçc nh·ªØng nh√† cung c·∫•p c√≥ √≠t nh·∫•t 5 ƒë∆°n h√†ng
        filtered_suppliers = supplier_df[supplier_df["total_orders"] >= 5].copy()
        
        if len(filtered_suppliers) < n_clusters:
            print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªß nh√† cung c·∫•p ƒë·ªÉ ph√¢n th√†nh {n_clusters} c·ª•m. Ch·ªâ c√≥ {len(filtered_suppliers)} seller ƒë·ªß ƒëi·ªÅu ki·ªán.")
            # Gi·∫£m s·ªë c·ª•m n·∫øu kh√¥ng ƒë·ªß d·ªØ li·ªáu
            n_clusters = max(2, len(filtered_suppliers) // 2)
            
        print(f"‚ÑπÔ∏è Ph√¢n c·ª•m {len(filtered_suppliers)} nh√† cung c·∫•p th√†nh {n_clusters} nh√≥m")

        # Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·ªÉ tr√°nh bias
        features = filtered_suppliers[["total_orders", "avg_shipping_days", "avg_freight"]].fillna(0)
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)

        # Ph√¢n c·ª•m
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        filtered_suppliers["cluster"] = kmeans.fit_predict(features_scaled)

        # Th√™m m√¥ t·∫£ c·ª•m
        cluster_stats = filtered_suppliers.groupby("cluster").agg({
            "avg_shipping_days": "mean",
            "avg_freight": "mean",
            "total_orders": "mean"
        })
        
        cluster_descriptions = {}
        for cluster_id, stats in cluster_stats.iterrows():
            if stats["avg_shipping_days"] < 15 and stats["avg_freight"] < 500000:
                description = "Nhanh v√† r·∫ª"
            elif stats["avg_shipping_days"] < 15 and stats["avg_freight"] >= 500000:
                description = "Nhanh nh∆∞ng ƒë·∫Øt"
            elif stats["avg_shipping_days"] >= 15 and stats["avg_freight"] < 500000:
                description = "Ch·∫≠m nh∆∞ng r·∫ª"
            else:
                description = "Ch·∫≠m v√† ƒë·∫Øt"
                
            cluster_descriptions[cluster_id] = description
            
        filtered_suppliers["cluster_description"] = filtered_suppliers["cluster"].map(cluster_descriptions)

        # Chuy·ªÉn th√†nh d·∫°ng dict ƒë·ªÉ l∆∞u v√† tr·∫£ v·ªÅ
        clusters = filtered_suppliers.to_dict(orient="records")
        
        # Cache v√† l∆∞u k·∫øt qu·∫£
        set_cache(cache_key, clusters, ttl_seconds=3600*24)
        save_supplier_clusters(clusters)
        
        print(f"‚úÖ Ho√†n th√†nh ph√¢n c·ª•m: {len(clusters)} nh√† cung c·∫•p")
        return clusters

    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh ph√¢n c·ª•m nh√† cung c·∫•p: {str(e)}")
        import traceback
        traceback.print_exc()
        return []


def analyze_bottlenecks(threshold_days=20):
    """
    Ph√¢n t√≠ch c√°c bottleneck trong quy tr√¨nh giao h√†ng, x√°c ƒë·ªãnh c√°c nh√† cung c·∫•p 
    c√≥ t·ª∑ l·ªá giao h√†ng tr·ªÖ cao
    """
    try:
        print("üöÄ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch bottleneck giao h√†ng...")
        cache_key = "shipping_bottlenecks"
        cached = get_cache(cache_key)
        if cached:
            return cached

        # Ki·ªÉm tra xem c√≥ n√™n d√πng Spark hay kh√¥ng
        use_spark = is_large_dataset()
        if use_spark:
            print("üìä S·ª≠ d·ª•ng Spark ƒë·ªÉ ph√¢n t√≠ch bottlenecks (d·ªØ li·ªáu l·ªõn)")
            from services.spark_analytics import analyze_bottlenecks_spark
            bottlenecks = analyze_bottlenecks_spark(threshold_days)
            
            # Cache v√† l∆∞u k·∫øt qu·∫£
            set_cache(cache_key, bottlenecks, ttl_seconds=3600*24)
            save_bottleneck_analysis(bottlenecks)
            
            return bottlenecks

        # N·∫øu kh√¥ng d√πng Spark, ti·∫øp t·ª•c v·ªõi code hi·ªán t·∫°i
        df = preprocess_data()

        # X√°c ƒë·ªãnh ƒë∆°n h√†ng n√†o b·ªã tr·ªÖ d·ª±a tr√™n ng∆∞·ª°ng
        df["is_late"] = df["shipping_duration"] > threshold_days
        
        print("üì¶ Th·ªëng k√™ shipping_duration:")
        print(df["shipping_duration"].describe())
        
        late_ratio_all = (df["is_late"].mean() * 100)
        print(f"‚ö†Ô∏è T·ª∑ l·ªá ƒë∆°n h√†ng b·ªã tr·ªÖ to√†n b·ªô theo ng∆∞·ª°ng {threshold_days} ng√†y: {late_ratio_all:.2f}%")

        # Ph√¢n t√≠ch theo t·ª´ng nh√† cung c·∫•p
        bottlenecks = df.groupby("seller_id").agg({
            "order_id": "count",
            "is_late": "mean",
            "product_category_name": lambda x: x.mode()[0] if not x.mode().empty else "Unknown",
            "shipping_duration": "mean"
        }).reset_index()

        bottlenecks.columns = ["seller_id", "total_orders", "late_ratio", "top_category", "avg_delivery_time"]

        # Ch·ªâ l·∫•y seller c√≥ √≠t nh·∫•t 5 ƒë∆°n v√† t·ª∑ l·ªá tr·ªÖ cao h∆°n trung b√¨nh
        bottlenecks = bottlenecks[(bottlenecks["total_orders"] >= 5) & 
                                  (bottlenecks["late_ratio"] > late_ratio_all/100)]

        # Th√™m th√¥ng tin ph·∫ßn trƒÉm
        bottlenecks["late_percentage"] = (bottlenecks["late_ratio"] * 100).round(1)
        
        # Th√™m ghi ch√∫ m·ª©c ƒë·ªô nghi√™m tr·ªçng
        def get_severity(row):
            if row["late_percentage"] > 75:
                return "R·∫•t nghi√™m tr·ªçng"
            elif row["late_percentage"] > 50:
                return "Nghi√™m tr·ªçng"
            elif row["late_percentage"] > 25:
                return "Trung b√¨nh"
            else:
                return "Nh·∫π"
        
        bottlenecks["severity"] = bottlenecks.apply(get_severity, axis=1)
        
        # L·∫•y 10 seller c√≥ v·∫•n ƒë·ªÅ nh·∫•t
        top_bottlenecks = bottlenecks.sort_values("late_percentage", ascending=False).head(10)
        
        # Chu·∫©n b·ªã k·∫øt qu·∫£
        top_bottlenecks_list = top_bottlenecks.to_dict(orient="records")
        
        # Cache v√† l∆∞u k·∫øt qu·∫£
        set_cache(cache_key, top_bottlenecks_list, ttl_seconds=3600*24)
        save_bottleneck_analysis(top_bottlenecks_list)
        
        print(f"‚úÖ Ho√†n th√†nh ph√¢n t√≠ch bottleneck: {len(top_bottlenecks_list)} seller c√≥ v·∫•n ƒë·ªÅ")
        return top_bottlenecks_list

    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch bottleneck: {str(e)}")
        import traceback
        traceback.print_exc()
        return []