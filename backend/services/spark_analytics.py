# services/spark_analytics.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, date_format, to_date, datediff
import pandas as pd
import numpy as np
import os
from datetime import datetime
from dateutil.relativedelta import relativedelta
import matplotlib.pyplot as plt
from utils.plot import fig_to_base64
from utils.currency import brl_to_vnd
from utils.cache import set_cache
from services.mongodb import save_forecast_result
import traceback

def init_spark():
    """
    Kh·ªüi t·∫°o Spark Session v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u cho local mode
    """
    try:
        spark = SparkSession.getActiveSession()
        if spark is not None:
            return spark
            
        # C·∫•u h√¨nh ƒë∆°n gi·∫£n h∆°n cho local mode
        return SparkSession.builder \
            .appName("SupplyChainAnalytics") \
            .config("spark.executor.memory", "1g") \
            .config("spark.driver.memory", "2g") \
            .config("spark.driver.maxResultSize", "1g") \
            .config("spark.python.worker.memory", "1g") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .config("spark.sql.execution.arrow.maxRecordsPerBatch", "10000") \
            .master("local[*]") \
            .getOrCreate()
    except Exception as e:
        print(f"‚ùå Error initializing Spark: {str(e)}")
        return None

def forecast_demand_spark(periods=6):
    """
    D·ª± b√°o demand s·ª≠ d·ª•ng Spark SQL v·ªõi thu·∫≠t to√°n ƒë∆°n gi·∫£n
    """
    print(f"üöÄ Starting Spark forecast_demand service ({periods} periods)...")
    try:
        spark = init_spark()
        if spark is None:
            print("‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o Spark, chuy·ªÉn sang s·ª≠ d·ª•ng Pandas")
            from services.forecast import forecast_demand
            return forecast_demand(periods)

        # Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Spark
        from services.preprocess import UPLOAD_FOLDER
        
        # ƒê·ªçc c√°c file CSV
        orders = spark.read.csv(os.path.join(UPLOAD_FOLDER, "df_Orders.csv"), 
                              header=True, inferSchema=True)
        
        # X·ª≠ l√Ω timestamps
        orders = orders.withColumn("order_purchase_timestamp", 
                                to_date(col("order_purchase_timestamp")))
        
        # T√≠nh to√°n s·ªë l∆∞·ª£ng ƒë∆°n theo th√°ng
        orders = orders.withColumn("order_month", 
                                date_format(col("order_purchase_timestamp"), "yyyy-MM"))
        
        # Group by month v√† ƒë·∫øm s·ªë l∆∞·ª£ng ƒë∆°n h√†ng
        monthly_orders = orders.groupBy("order_month").count().orderBy("order_month")
        
        # Chuy·ªÉn sang Pandas
        monthly_orders_pd = monthly_orders.toPandas()
        monthly_orders_pd["order_month"] = pd.to_datetime(monthly_orders_pd["order_month"])
        
        # ƒê√≥ng Spark session
        spark.stop()
        
        # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n ƒë·ªÉ d·ª± b√°o
        monthly_series = monthly_orders_pd.set_index("order_month")["count"]
        
        # ƒê·∫£m b·∫£o d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß c√°c th√°ng
        monthly_series = monthly_series.resample('MS').asfreq().fillna(method='ffill').fillna(0)
        
        # D·ª± b√°o ƒë∆°n gi·∫£n b·∫±ng c√°ch s·ª≠ d·ª•ng trung b√¨nh ƒë·ªông
        last_values = monthly_series.tail(3)
        avg_growth = last_values.pct_change().mean()
        
        if pd.isna(avg_growth) or abs(avg_growth) > 0.5:  # N·∫øu tƒÉng tr∆∞·ªüng b·∫•t th∆∞·ªùng
            avg_growth = 0.05  # Gi·∫£ ƒë·ªãnh tƒÉng tr∆∞·ªüng 5%
            
        # D·ª± b√°o
        last_value = monthly_series.iloc[-1]
        forecast_xgb = []
        
        for i in range(1, periods + 1):
            next_value = last_value * (1 + avg_growth)
            forecast_xgb.append(int(max(100, next_value)))
            last_value = next_value
            
        # D·ª± b√°o ARIMA ƒë∆°n gi·∫£n (s·ª≠ d·ª•ng trung b√¨nh)
        forecast_arima = [int(monthly_series.mean()) for _ in range(periods)]
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu k·∫øt qu·∫£
        last_date = monthly_series.index[-1]
        future_dates = [last_date + relativedelta(months=i) for i in range(1, periods + 1)]
        forecast_df = pd.DataFrame({
            "month": [d.strftime("%Y-%m") for d in future_dates],
            "xgboost": forecast_xgb,
            "arima": forecast_arima,
        })
        
        # T·∫°o d·ªØ li·ªáu cho bi·ªÉu ƒë·ªì
        chart_data = [{"month": date.strftime("%Y-%m"), "orders": int(val), "type": "Th·ª±c t·∫ø"} 
                     for date, val in monthly_series.items()]
        chart_data += [{"month": date.strftime("%Y-%m"), "orders": val, "type": "XGBoost"} 
                      for date, val in zip(future_dates, forecast_xgb)]
        chart_data += [{"month": date.strftime("%Y-%m"), "orders": val, "type": "ARIMA"} 
                      for date, val in zip(future_dates, forecast_arima)]
        
        # V·∫Ω bi·ªÉu ƒë·ªì
        fig, ax = plt.subplots(figsize=(10, 4))
        monthly_series.plot(ax=ax, label="Th·ª±c t·∫ø", marker="o")
        pd.Series(forecast_xgb, index=future_dates).plot(ax=ax, label="XGBoost-Spark", linestyle="--", marker="x")
        pd.Series(forecast_arima, index=future_dates).plot(ax=ax, label="ARIMA-Spark", linestyle="--", marker="s")
        ax.set_title("So s√°nh d·ª± b√°o ƒë∆°n h√†ng (Spark simplified)")
        ax.set_ylabel("S·ªë ƒë∆°n")
        ax.set_xlabel("Th√°ng")
        ax.legend()
        ax.grid(True)
        
        # L∆∞u k·∫øt qu·∫£ v√†o MongoDB
        save_forecast_result({
            "category": "T·ªïng th·ªÉ",
            "model": "Spark (simplified)",
            "forecast_table": forecast_df.to_dict(orient="records"),
            "mae_rmse_comparison": {
                "xgboost": {"mae": 0, "rmse": 0},  # Kh√¥ng t√≠nh metrics th·ª±c t·∫ø
                "arima": {"mae": 0, "rmse": 0}
            }
        })
        
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£
        return {
            "status": "success",
            "category": "T·ªïng th·ªÉ",
            "forecast_table": forecast_df.to_dict(orient="records"),
            "chart_data": chart_data,
            "chart": fig_to_base64(fig),
            "mae_rmse_comparison": {
                "xgboost": {"mae": 0, "rmse": 0},
                "arima": {"mae": 0, "rmse": 0}
            }
        }
    except Exception as e:
        print(f"‚ùå Error in forecast_demand_spark: {str(e)}")
        print(traceback.format_exc())
        # Fallback to pandas
        from services.forecast import forecast_demand
        return forecast_demand(periods)

def forecast_demand_by_category_spark(category_name, periods=6):
    """
    D·ª± b√°o nhu c·∫ßu theo danh m·ª•c s·ª≠ d·ª•ng Spark SQL ƒë∆°n gi·∫£n
    """
    print(f"üöÄ Spark Forecasting for category: {category_name}")
    try:
        spark = init_spark()
        if spark is None:
            print("‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o Spark, chuy·ªÉn sang s·ª≠ d·ª•ng Pandas")
            from services.forecast import forecast_demand_by_category
            return forecast_demand_by_category(category_name, periods)

        # Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Spark
        from services.preprocess import UPLOAD_FOLDER
        
        # ƒê·ªçc c√°c file CSV
        orders = spark.read.csv(os.path.join(UPLOAD_FOLDER, "df_Orders.csv"), 
                              header=True, inferSchema=True)
        order_items = spark.read.csv(os.path.join(UPLOAD_FOLDER, "df_OrderItems.csv"), 
                                   header=True, inferSchema=True)
        products = spark.read.csv(os.path.join(UPLOAD_FOLDER, "df_Products.csv"), 
                                header=True, inferSchema=True)
        
        # X·ª≠ l√Ω timestamps
        orders = orders.withColumn("order_purchase_timestamp", 
                                to_date(col("order_purchase_timestamp")))
        
        # T√≠nh to√°n s·ªë l∆∞·ª£ng ƒë∆°n theo th√°ng
        orders = orders.withColumn("order_month", 
                                date_format(col("order_purchase_timestamp"), "yyyy-MM"))
        
        # Join ƒë·ªÉ l·∫•y th√¥ng tin s·∫£n ph·∫©m
        df = order_items.join(orders, "order_id", "inner").join(products, "product_id", "left")
        
        # L·ªçc theo danh m·ª•c
        df_cat = df.filter(col("product_category_name") == category_name)
        
        # Ki·ªÉm tra xem c√≥ ƒë·ªß d·ªØ li·ªáu kh√¥ng
        if df_cat.count() < 10:
            print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho {category_name}, chuy·ªÉn sang d·ªØ li·ªáu m·∫´u")
            from services.forecast import forecast_demand_by_category
            return forecast_demand_by_category(category_name, periods)
        
        # Group by month v√† ƒë·∫øm s·ªë l∆∞·ª£ng ƒë∆°n h√†ng cho danh m·ª•c c·ª• th·ªÉ
        monthly_orders = df_cat.groupBy("order_month").count().orderBy("order_month")
        
        # Chuy·ªÉn sang Pandas
        monthly_orders_pd = monthly_orders.toPandas()
        monthly_orders_pd["order_month"] = pd.to_datetime(monthly_orders_pd["order_month"])
        
        # ƒê√≥ng Spark session
        spark.stop()
        
        # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n ƒë·ªÉ d·ª± b√°o
        monthly_series = monthly_orders_pd.set_index("order_month")["count"]
        
        # ƒê·∫£m b·∫£o d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß c√°c th√°ng
        monthly_series = monthly_series.resample('MS').asfreq().fillna(method='ffill').fillna(0)
        
        # D·ª± b√°o ƒë∆°n gi·∫£n b·∫±ng c√°ch s·ª≠ d·ª•ng trung b√¨nh ƒë·ªông
        last_values = monthly_series.tail(3)
        avg_growth = last_values.pct_change().mean()
        
        if pd.isna(avg_growth) or abs(avg_growth) > 0.5:  # N·∫øu tƒÉng tr∆∞·ªüng b·∫•t th∆∞·ªùng
            avg_growth = 0.05  # Gi·∫£ ƒë·ªãnh tƒÉng tr∆∞·ªüng 5%
            
        # D·ª± b√°o
        last_value = monthly_series.iloc[-1]
        forecast_xgb = []
        
        for i in range(1, periods + 1):
            next_value = last_value * (1 + avg_growth)
            forecast_xgb.append(int(max(0, next_value)))
            last_value = next_value
            
        # D·ª± b√°o ARIMA ƒë∆°n gi·∫£n (s·ª≠ d·ª•ng trung b√¨nh)
        forecast_arima = [int(monthly_series.mean()) for _ in range(periods)]
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu k·∫øt qu·∫£
        last_date = monthly_series.index[-1]
        future_dates = [last_date + relativedelta(months=i) for i in range(1, periods + 1)]
        forecast_df = pd.DataFrame({
            "month": [d.strftime("%Y-%m") for d in future_dates],
            "xgboost": forecast_xgb,
            "arima": forecast_arima,
        })
        
        # T·∫°o d·ªØ li·ªáu cho bi·ªÉu ƒë·ªì
        chart_data = [{"month": date.strftime("%Y-%m"), "orders": int(val), "type": "Th·ª±c t·∫ø", "category": category_name} 
                     for date, val in monthly_series.items()]
        chart_data += [{"month": date.strftime("%Y-%m"), "orders": val, "type": "XGBoost", "category": category_name} 
                      for date, val in zip(future_dates, forecast_xgb)]
        chart_data += [{"month": date.strftime("%Y-%m"), "orders": val, "type": "ARIMA", "category": category_name} 
                      for date, val in zip(future_dates, forecast_arima)]
        
        # T√≠nh optimal inventory v√† holding cost
        optimal_inventory = int(np.max(forecast_xgb)) if forecast_xgb else 0
        unit_holding_cost = brl_to_vnd(5)
        holding_cost = optimal_inventory * unit_holding_cost
        
        # L∆∞u k·∫øt qu·∫£ v√†o MongoDB
        save_forecast_result({
            "category": category_name,
            "model": "Spark (simplified)",
            "forecast_table": forecast_df.to_dict(orient="records"),
            "optimal_inventory": optimal_inventory,
            "holding_cost": holding_cost,
            "mae_rmse_comparison": {
                "xgboost": {"mae": 0, "rmse": 0},
                "arima": {"mae": 0, "rmse": 0}
            }
        })
        
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£
        return {
            "status": "success",
            "category": category_name,
            "forecast_table": forecast_df.to_dict(orient="records"),
            "chart_data": chart_data,
            "optimal_inventory": optimal_inventory,
            "holding_cost": holding_cost,
            "mae_rmse_comparison": {
                "xgboost": {"mae": 0, "rmse": 0},
                "arima": {"mae": 0, "rmse": 0}
            }
        }
    except Exception as e:
        print(f"‚ùå Error in forecast_demand_by_category_spark({category_name}): {str(e)}")
        print(traceback.format_exc())
        # Fallback to pandas
        from services.forecast import forecast_demand_by_category
        return forecast_demand_by_category(category_name, periods)

def forecast_all_categories_spark(limit=15):
    """
    D·ª± b√°o cho nhi·ªÅu danh m·ª•c ƒë·ªìng th·ªùi s·ª≠ d·ª•ng Spark SQL ƒë∆°n gi·∫£n
    """
    print(f"üöÄ Starting Spark forecast for all categories (limit: {limit})...")
    try:
        spark = init_spark()
        if spark is None:
            print("‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o Spark, chuy·ªÉn sang s·ª≠ d·ª•ng Pandas")
            # Fallback to pandas
            return []
            
        # ƒê·ªçc c√°c file CSV
        from services.preprocess import UPLOAD_FOLDER
        products = spark.read.csv(os.path.join(UPLOAD_FOLDER, "df_Products.csv"), 
                                header=True, inferSchema=True)
        
        # L·∫•y danh s√°ch c√°c danh m·ª•c
        categories = products.select("product_category_name").distinct().filter(col("product_category_name").isNotNull())
        categories_list = [row["product_category_name"] for row in categories.take(limit)]
        
        # ƒê√≥ng spark session
        spark.stop()
        
        # X·ª≠ l√Ω t·ª´ng danh m·ª•c
        all_forecasts = []
        for category in categories_list:
            try:
                result = forecast_demand_by_category_spark(category)
                if result.get("status") == "success":
                    all_forecasts.append(result)
            except Exception as e:
                print(f"‚ùå Error in Spark forecast for category {category}: {str(e)}")
                
        # Th√™m d·ª± b√°o t·ªïng th·ªÉ
        overall = forecast_demand_spark()
        if overall.get("status") == "success":
            all_forecasts.insert(0, overall)
            
        print(f"üìä T·ªïng c·ªông {len(all_forecasts) - 1}/{len(categories_list)} danh m·ª•c c√≥ d·ª± b√°o th√†nh c√¥ng v·ªõi Spark")
        return all_forecasts
        
    except Exception as e:
        print(f"‚ùå Error in forecast_all_categories_spark: {str(e)}")
        print(traceback.format_exc())
        return []

# C√°c h√†m kh√°c v·ªõi logic ƒë∆°n gi·∫£n h∆°n
def cluster_suppliers_spark(n_clusters=3):
    """Phi√™n b·∫£n ƒë∆°n gi·∫£n h√≥a c·ªßa ph√¢n c·ª•m nh√† cung c·∫•p"""
    try:
        from services.reorder import cluster_suppliers
        return cluster_suppliers(n_clusters)
    except Exception as e:
        print(f"‚ùå Error in cluster_suppliers_spark: {str(e)}")
        return []

def analyze_bottlenecks_spark(threshold_days=20):
    """Phi√™n b·∫£n ƒë∆°n gi·∫£n h√≥a c·ªßa ph√¢n t√≠ch bottlenecks"""
    try:
        from services.reorder import analyze_bottlenecks
        return analyze_bottlenecks(threshold_days)
    except Exception as e:
        print(f"‚ùå Error in analyze_bottlenecks_spark: {str(e)}")
        return []

def calculate_reorder_strategy_spark():
    """Phi√™n b·∫£n ƒë∆°n gi·∫£n h√≥a c·ªßa calculate_reorder_strategy"""
    try:
        from services.reorder import calculate_reorder_strategy
        return calculate_reorder_strategy()
    except Exception as e:
        print(f"‚ùå Error in calculate_reorder_strategy_spark: {str(e)}")
        return []

# C√°c h√†m helper cho charts
def get_top_reorder_points_spark(limit=10):
    result = calculate_reorder_strategy_spark()
    if not result:
        return []
    top_reorder = sorted(result, key=lambda x: x['reorder_point'], reverse=True)[:limit]
    return [{"category": item["category"], "value": item["reorder_point"]} for item in top_reorder]

def get_top_safety_stock_spark(limit=10):
    result = calculate_reorder_strategy_spark()
    if not result:
        return []
    top_ss = sorted(result, key=lambda x: x['safety_stock'], reverse=True)[:limit]
    return [{"category": item["category"], "value": item["safety_stock"]} for item in top_ss]

def get_top_lead_time_spark(limit=10):
    result = calculate_reorder_strategy_spark()
    if not result:
        return []
    top_lt = sorted(result, key=lambda x: x['avg_lead_time_days'], reverse=True)[:limit]
    return [{"category": item["category"], "value": round(item["avg_lead_time_days"], 1)} for item in top_lt]

def get_top_optimal_inventory_spark(limit=10):
    result = calculate_reorder_strategy_spark()
    if not result:
        return []
    top_inventory = sorted(result, key=lambda x: x['optimal_inventory'], reverse=True)[:limit]
    return [{"category": item["category"], "value": item["optimal_inventory"]} for item in top_inventory]

def get_top_holding_cost_spark(limit=10):
    result = calculate_reorder_strategy_spark()
    if not result:
        return []
    top_cost = sorted(result, key=lambda x: x['holding_cost'], reverse=True)[:limit]
    return [{"category": item["category"], "value": item["holding_cost"]} for item in top_cost]

def get_top_potential_saving_spark(limit=10):
    # Fallback to non-Spark version
    from services.reorder import generate_optimization_recommendations, calculate_reorder_strategy
    strategy = calculate_reorder_strategy()
    recommendations_df = generate_optimization_recommendations(strategy, return_df=True)
    
    if recommendations_df.empty or "potential_saving" not in recommendations_df.columns:
        return []
        
    top_save = recommendations_df.sort_values("potential_saving", ascending=False).head(limit)
    return [{"category": row["category"], "value": row["potential_saving"]} for _, row in top_save.iterrows()]

def generate_optimization_recommendations_spark():
    # Fallback to non-Spark version
    from services.reorder import generate_optimization_recommendations, calculate_reorder_strategy
    strategy = calculate_reorder_strategy()
    return generate_optimization_recommendations(strategy)